# GANs
ğˆğ¦ğ©ğ¥ğğ¦ğğ§ğšğ­ğšğ­ğ¢ğ¨ğ§ ğ¨ğŸ ğ­ğ¡ğ¢ğ¬ ğ©ğšğ©ğğ« -- https://arxiv.org/abs/1511.06434
ğ¦ğ¨ğğğ¥ ğ ğğ§ğğ«ğšğ­ğ ğŸ•ğŸ‘ ğ± ğŸ•ğŸ‘ ğ©ğ¢ğ±ğğ¥ ğ©ğğ¨ğ©ğ¥ğğ¬ ğ¢ğ¦ğšğ ğğ¬

The authors of DCGAN offer us a family of convolutional neural network models that provide much more stable training in the context of GANs. In the 'model.py' file, you can find the proposed architecture, with almost everything matching the description in the paper. For example, they use Leaky Relu in the Discriminator, Batchnorms in both the Discriminator and Generator, ReLU in the Generator, and so on. The only change I made was to the generator architecture; I added one more Deconvolution layer before the final layer.

In the GAN paper, which is available at https://arxiv.org/abs/1406.2661, the authors proposed an algorithm to train models based on the following loss function:

min G max D ğ‹(ğƒ, ğ†) = ğ„[ğ¥ğ¨ğ (ğƒ(ğ±))] + ğ„[ğ¥ğ¨ğ (1 âˆ’ ğƒ(ğ†(ğ³)))],

where ğ± is sampled from the data distribution ğ©ğğšğ­ğš(ğ±), and ğ³ is sampled from the noise distribution ğ©ğ³(ğ³). The training process involves K steps to update D and one step to update G. In my implementation, I set K=1, and I sample only one noisy vector from a Normal distribution, as opposed to a uniform distribution, for updating the weights of both networks.

ğ“ğ«ğšğ¢ğ§ ğœğ¨ğ§ğŸğ¢ğ ğ¬:
Batch_size = 128
G_opt = Adam lr=0.0002, beta_1=0.5
D_opt = Adam lr=0.0002, beta_1=0.5
Leaky(alpha=0.2)
k_steps_to_train_D = 1

ğ‘ğ„ğ’ğ”ğ‹ğ“ğ’
![results](https://github.com/Areg147/GANs/assets/131033594/7f0bb009-cd45-493a-870f-b8a13ba41508)
